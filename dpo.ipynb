{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./jp-py310/lib/python3.10/site-packages (4.30.1)\n",
      "Collecting trl\n",
      "  Downloading trl-0.7.4-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.9/133.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting wandb\n",
      "  Downloading wandb-0.16.0-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in ./jp-py310/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./jp-py310/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./jp-py310/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./jp-py310/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./jp-py310/lib/python3.10/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./jp-py310/lib/python3.10/site-packages (from transformers) (1.25.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./jp-py310/lib/python3.10/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in ./jp-py310/lib/python3.10/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: filelock in ./jp-py310/lib/python3.10/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./jp-py310/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: datasets in ./jp-py310/lib/python3.10/site-packages (from trl) (2.13.1)\n",
      "Collecting tyro>=0.5.11\n",
      "  Downloading tyro-0.6.0-py3-none-any.whl (100 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: accelerate in ./jp-py310/lib/python3.10/site-packages (from trl) (0.21.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in ./jp-py310/lib/python3.10/site-packages (from trl) (2.0.1)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: setuptools in ./jp-py310/lib/python3.10/site-packages (from wandb) (65.5.0)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in ./jp-py310/lib/python3.10/site-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in ./jp-py310/lib/python3.10/site-packages (from wandb) (8.1.5)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.38.0-py2.py3-none-any.whl (252 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: appdirs>=1.4.3 in ./jp-py310/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in ./jp-py310/lib/python3.10/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in ./jp-py310/lib/python3.10/site-packages (from wandb) (3.1.32)\n",
      "Requirement already satisfied: six>=1.4.0 in ./jp-py310/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./jp-py310/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: fsspec in ./jp-py310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./jp-py310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./jp-py310/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./jp-py310/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./jp-py310/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./jp-py310/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./jp-py310/lib/python3.10/site-packages (from torch>=1.4.0->trl) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./jp-py310/lib/python3.10/site-packages (from torch>=1.4.0->trl) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./jp-py310/lib/python3.10/site-packages (from torch>=1.4.0->trl) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./jp-py310/lib/python3.10/site-packages (from torch>=1.4.0->trl) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./jp-py310/lib/python3.10/site-packages (from torch>=1.4.0->trl) (11.7.99)\n",
      "Requirement already satisfied: jinja2 in ./jp-py310/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1.2)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./jp-py310/lib/python3.10/site-packages (from torch>=1.4.0->trl) (10.2.10.91)\n",
      "Requirement already satisfied: networkx in ./jp-py310/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./jp-py310/lib/python3.10/site-packages (from torch>=1.4.0->trl) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./jp-py310/lib/python3.10/site-packages (from torch>=1.4.0->trl) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in ./jp-py310/lib/python3.10/site-packages (from torch>=1.4.0->trl) (2.0.0)\n",
      "Requirement already satisfied: sympy in ./jp-py310/lib/python3.10/site-packages (from torch>=1.4.0->trl) (1.12)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./jp-py310/lib/python3.10/site-packages (from torch>=1.4.0->trl) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./jp-py310/lib/python3.10/site-packages (from torch>=1.4.0->trl) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./jp-py310/lib/python3.10/site-packages (from torch>=1.4.0->trl) (11.10.3.66)\n",
      "Requirement already satisfied: wheel in ./jp-py310/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->trl) (0.40.0)\n",
      "Requirement already satisfied: cmake in ./jp-py310/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.4.0->trl) (3.26.4)\n",
      "Requirement already satisfied: lit in ./jp-py310/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.4.0->trl) (16.0.6)\n",
      "Collecting docstring-parser>=0.14.1\n",
      "  Using cached docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Collecting rich>=11.1.0\n",
      "  Downloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.6/240.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting shtab>=1.5.6\n",
      "  Downloading shtab-1.6.5-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in ./jp-py310/lib/python3.10/site-packages (from datasets->trl) (0.3.6)\n",
      "Requirement already satisfied: pandas in ./jp-py310/lib/python3.10/site-packages (from datasets->trl) (2.0.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./jp-py310/lib/python3.10/site-packages (from datasets->trl) (12.0.1)\n",
      "Requirement already satisfied: multiprocess in ./jp-py310/lib/python3.10/site-packages (from datasets->trl) (0.70.14)\n",
      "Requirement already satisfied: xxhash in ./jp-py310/lib/python3.10/site-packages (from datasets->trl) (3.2.0)\n",
      "Requirement already satisfied: aiohttp in ./jp-py310/lib/python3.10/site-packages (from datasets->trl) (3.8.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./jp-py310/lib/python3.10/site-packages (from aiohttp->datasets->trl) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./jp-py310/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./jp-py310/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./jp-py310/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./jp-py310/lib/python3.10/site-packages (from aiohttp->datasets->trl) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./jp-py310/lib/python3.10/site-packages (from aiohttp->datasets->trl) (6.0.4)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./jp-py310/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./jp-py310/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.15.1)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in ./jp-py310/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./jp-py310/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./jp-py310/lib/python3.10/site-packages (from pandas->datasets->trl) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./jp-py310/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./jp-py310/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: shtab, setproctitle, sentry-sdk, mdurl, docstring-parser, docker-pycreds, markdown-it-py, wandb, rich, tyro, trl\n",
      "  Attempting uninstall: docstring-parser\n",
      "    Found existing installation: docstring-parser 0.8.1\n",
      "    Uninstalling docstring-parser-0.8.1:\n",
      "      Successfully uninstalled docstring-parser-0.8.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchx-nightly 2023.6.1 requires docstring-parser==0.8.1, but you have docstring-parser 0.15 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed docker-pycreds-0.4.0 docstring-parser-0.15 markdown-it-py-3.0.0 mdurl-0.1.2 rich-13.7.0 sentry-sdk-1.38.0 setproctitle-1.3.3 shtab-1.6.5 trl-0.7.4 tyro-0.6.0 wandb-0.16.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers trl wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "\n",
    "from trl import DPOTrainer\n",
    "from trl.core import LengthSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(config, dataset_name=\"imdb\", input_min_text_length=2, input_max_text_length=8):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # load imdb with datasets\n",
    "    ds = load_dataset(dataset_name, split=\"train\")\n",
    "    ds = ds.rename_columns({\"text\": \"review\"})\n",
    "    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
    "\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=\"lvwerra/gpt2-imdb\",\n",
    "    learning_rate=1.41e-5,\n",
    "    log_with=\"wandb\",\n",
    ")\n",
    "\n",
    "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/jpsingh/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Loading cached processed dataset at /home/jpsingh/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-cd96d6dda9f74d63.arrow\n",
      "Loading cached processed dataset at /home/jpsingh/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-fd42dc5906724bf4.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = build_dataset(config)\n",
    "\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24895, 21171)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['query']), len(set(dataset['query']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': ['I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       "  '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.',\n",
       "  \"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\",\n",
       "  \"This film was probably inspired by Godard's Masculin, féminin and I urge you to see that film instead.<br /><br />The film has two strong elements and those are, (1) the realistic acting (2) the impressive, undeservedly good, photo. Apart from that, what strikes me most is the endless stream of silliness. Lena Nyman has to be most annoying actress in the world. She acts so stupid and with all the nudity in this film,...it's unattractive. Comparing to Godard's film, intellectuality has been replaced with stupidity. Without going too far on this subject, I would say that follows from the difference in ideals between the French and the Swedish society.<br /><br />A movie of its time, and place. 2/10.\",\n",
       "  'Oh, brother...after hearing about this ridiculous film for umpteen years all I can think of is that old Peggy Lee song..<br /><br />\"Is that all there is??\" ...I was just an early teen when this smoked fish hit the U.S. I was too young to get in the theater (although I did manage to sneak into \"Goodbye Columbus\"). Then a screening at a local film museum beckoned - Finally I could see this film, except now I was as old as my parents were when they schlepped to see it!!<br /><br />The ONLY reason this film was not condemned to the anonymous sands of time was because of the obscenity case sparked by its U.S. release. MILLIONS of people flocked to this stinker, thinking they were going to see a sex film...Instead, they got lots of closeups of gnarly, repulsive Swedes, on-street interviews in bland shopping malls, asinie political pretension...and feeble who-cares simulated sex scenes with saggy, pale actors.<br /><br />Cultural icon, holy grail, historic artifact..whatever this thing was, shred it, burn it, then stuff the ashes in a lead box!<br /><br />Elite esthetes still scrape to find value in its boring pseudo revolutionary political spewings..But if it weren\\'t for the censorship scandal, it would have been ignored, then forgotten.<br /><br />Instead, the \"I Am Blank, Blank\" rhythymed title was repeated endlessly for years as a titilation for porno films (I am Curious, Lavender - for gay films, I Am Curious, Black - for blaxploitation films, etc..) and every ten years or so the thing rises from the dead, to be viewed by a new generation of suckers who want to see that \"naughty sex film\" that \"revolutionized the film industry\"...<br /><br />Yeesh, avoid like the plague..Or if you MUST see it - rent the video and fast forward to the \"dirty\" parts, just to get it over with.<br /><br />',\n",
       "  \"I would put this at the top of my list of films in the category of unwatchable trash! There are films that are bad, but the worst kind are the ones that are unwatchable but you are suppose to like them because they are supposed to be good for you! The sex sequences, so shocking in its day, couldn't even arouse a rabbit. The so called controversial politics is strictly high school sophomore amateur night Marxism. The film is self-consciously arty in the worst sense of the term. The photography is in a harsh grainy black and white. Some scenes are out of focus or taken from the wrong angle. Even the sound is bad! And some people call this art?<br /><br />\",\n",
       "  \"Whoever wrote the screenplay for this movie obviously never consulted any books about Lucille Ball, especially her autobiography. I've never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi. I could write a whole list of factual errors, but it would go on for pages. In all, I believe that Lucille Ball is one of those inimitable people who simply cannot be portrayed by anyone other than themselves. If I were Lucie Arnaz and Desi, Jr., I would be irate at how many mistakes were made in this film. The filmmakers tried hard, but the movie seems awfully sloppy to me.\",\n",
       "  'When I first saw a glimpse of this movie, I quickly noticed the actress who was playing the role of Lucille Ball. Rachel York\\'s portrayal of Lucy is absolutely awful. Lucille Ball was an astounding comedian with incredible talent. To think about a legend like Lucille Ball being portrayed the way she was in the movie is horrendous. I cannot believe out of all the actresses in the world who could play a much better Lucy, the producers decided to get Rachel York. She might be a good actress in other roles but to play the role of Lucille Ball is tough. It is pretty hard to find someone who could resemble Lucille Ball, but they could at least find someone a bit similar in looks and talent. If you noticed York\\'s portrayal of Lucy in episodes of I Love Lucy like the chocolate factory or vitavetavegamin, nothing is similar in any way-her expression, voice, or movement.<br /><br />To top it all off, Danny Pino playing Desi Arnaz is horrible. Pino does not qualify to play as Ricky. He\\'s small and skinny, his accent is unreal, and once again, his acting is unbelievable. Although Fred and Ethel were not similar either, they were not as bad as the characters of Lucy and Ricky.<br /><br />Overall, extremely horrible casting and the story is badly told. If people want to understand the real life situation of Lucille Ball, I suggest watching A&E Biography of Lucy and Desi, read the book from Lucille Ball herself, or PBS\\' American Masters: Finding Lucy. If you want to see a docudrama, \"Before the Laughter\" would be a better choice. The casting of Lucille Ball and Desi Arnaz in \"Before the Laughter\" is much better compared to this. At least, a similar aspect is shown rather than nothing.',\n",
       "  'Who are these \"They\"- the actors? the filmmakers? Certainly couldn\\'t be the audience- this is among the most air-puffed productions in existence. It\\'s the kind of movie that looks like it was a lot of fun to shoot\\x97 TOO much fun, nobody is getting any actual work done, and that almost always makes for a movie that\\'s no fun to watch.<br /><br />Ritter dons glasses so as to hammer home his character\\'s status as a sort of doppleganger of the bespectacled Bogdanovich; the scenes with the breezy Ms. Stratten are sweet, but have an embarrassing, look-guys-I\\'m-dating-the-prom-queen feel to them. Ben Gazzara sports his usual cat\\'s-got-canary grin in a futile attempt to elevate the meager plot, which requires him to pursue Audrey Hepburn with all the interest of a narcoleptic at an insomnia clinic. In the meantime, the budding couple\\'s respective children (nepotism alert: Bogdanovich\\'s daughters) spew cute and pick up some fairly disturbing pointers on \\'love\\' while observing their parents. (Ms. Hepburn, drawing on her dignity, manages to rise above the proceedings- but she has the monumental challenge of playing herself, ostensibly.) Everybody looks great, but so what? It\\'s a movie and we can expect that much, if that\\'s what you\\'re looking for you\\'d be better off picking up a copy of Vogue.<br /><br />Oh- and it has to be mentioned that Colleen Camp thoroughly annoys, even apart from her singing, which, while competent, is wholly unconvincing... the country and western numbers are woefully mismatched with the standards on the soundtrack. Surely this is NOT what Gershwin (who wrote the song from which the movie\\'s title is derived) had in mind; his stage musicals of the 20\\'s may have been slight, but at least they were long on charm. \"They All Laughed\" tries to coast on its good intentions, but nobody- least of all Peter Bogdanovich - has the good sense to put on the brakes.<br /><br />Due in no small part to the tragic death of Dorothy Stratten, this movie has a special place in the heart of Mr. Bogdanovich- he even bought it back from its producers, then distributed it on his own and went bankrupt when it didn\\'t prove popular. His rise and fall is among the more sympathetic and tragic of Hollywood stories, so there\\'s no joy in criticizing the film... there _is_ real emotional investment in Ms. Stratten\\'s scenes. But \"Laughed\" is a faint echo of \"The Last Picture Show\", \"Paper Moon\" or \"What\\'s Up, Doc\"- following \"Daisy Miller\" and \"At Long Last Love\", it was a thundering confirmation of the phase from which P.B. has never emerged.<br /><br />All in all, though, the movie is harmless, only a waste of rental. I want to watch people having a good time, I\\'ll go to the park on a sunny day. For filmic expressions of joy and love, I\\'ll stick to Ernest Lubitsch and Jaques Demy...',\n",
       "  \"This is said to be a personal film for Peter Bogdonavitch. He based it on his life but changed things around to fit the characters, who are detectives. These detectives date beautiful models and have no problem getting them. Sounds more like a millionaire playboy filmmaker than a detective, doesn't it? This entire movie was written by Peter, and it shows how out of touch with real people he was. You're supposed to write what you know, and he did that, indeed. And leaves the audience bored and confused, and jealous, for that matter. This is a curio for people who want to see Dorothy Stratten, who was murdered right after filming. But Patti Hanson, who would, in real life, marry Keith Richards, was also a model, like Stratten, but is a lot better and has a more ample part. In fact, Stratten's part seemed forced; added. She doesn't have a lot to do with the story, which is pretty convoluted to begin with. All in all, every character in this film is somebody that very few people can relate with, unless you're millionaire from Manhattan with beautiful supermodels at your beckon call. For the rest of us, it's an irritating snore fest. That's what happens when you're out of touch. You entertain your few friends with inside jokes, and bore all the rest.\"],\n",
       " 'label': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'input_ids': [tensor([   40, 26399,   314,  3001,   327, 47269, 20958]),\n",
       "  tensor([    1,    40,  1703, 44269,    25, 12550]),\n",
       "  tensor([1532,  691,  284, 3368, 1642,  428]),\n",
       "  tensor([1212, 2646]),\n",
       "  tensor([5812,   11, 3956,  986]),\n",
       "  tensor([ 40, 561]),\n",
       "  tensor([47896,  2630,   262, 39769,   329]),\n",
       "  tensor([ 2215,   314,   717,  2497,   257, 19350]),\n",
       "  tensor([8241,  389,  777,  366]),\n",
       "  tensor([1212,  318])],\n",
       " 'query': ['I rented I AM CURIOUS',\n",
       "  '\"I Am Curious: Yellow',\n",
       "  'If only to avoid making this',\n",
       "  'This film',\n",
       "  'Oh, brother...',\n",
       "  'I would',\n",
       "  'Whoever wrote the screenplay for',\n",
       "  'When I first saw a glimpse',\n",
       "  'Who are these \"',\n",
       "  'This is']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['review', 'label', 'input_ids', 'query'],\n",
       "    num_rows: 24895\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each question generate two answers. Then run a bert classifier to get the sentiment score.\n",
    "# Prefer the one with more positive sentiment.\n",
    "\n",
    "# questions = set()\n",
    "# for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "#     query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "#     #### Get response from gpt2\n",
    "#     response_tensors = []\n",
    "#     for query in query_tensors:\n",
    "#         gen_len = output_length_sampler()\n",
    "#         generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "#         response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "#         response_tensors.append(response.squeeze()[-gen_len:])\n",
    "#     batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "#     #### Compute sentiment score\n",
    "#     texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "#     pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
    "#     rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "\n",
    "#     #### Run PPO step\n",
    "#     stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "#     ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msingh-japinder\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/fsx-Training/shopqa-training-fsx-prod-us-east-1/jpsingh/shopqa/retrieval/bin/wandb/run-20231203_043641-bfsz329y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/singh-japinder/trl/runs/bfsz329y' target=\"_blank\">jolly-music-6</a></strong> to <a href='https://wandb.ai/singh-japinder/trl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/singh-japinder/trl' target=\"_blank\">https://wandb.ai/singh-japinder/trl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/singh-japinder/trl/runs/bfsz329y' target=\"_blank\">https://wandb.ai/singh-japinder/trl/runs/bfsz329y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"lvwerra/distilbert-imdb\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<trl.core.LengthSampler at 0x7f903c43af20>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_min_length = 4\n",
    "output_max_length = 16\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "output_length_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_len = output_length_sampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([   40, 26399,   314,  3001,   327, 47269, 20958]),\n",
       " tensor([    1,    40,  1703, 44269,    25, 12550]),\n",
       " tensor([1532,  691,  284, 3368, 1642,  428]),\n",
       " tensor([1212, 2646])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:4]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([   40, 26399,   314,  3001,   327, 47269, 20958,   257,  1178,  2745,\n",
       "          2084,   379,  3776, 19485,   810,   428], device='cuda:0'),\n",
       " tensor([    1,    40,  1703, 44269,    25, 12550,  2097,     1,   318,  1231,\n",
       "           257,  4719,   530,   286,   616], device='cuda:0'),\n",
       " tensor([ 1532,   691,   284,  3368,  1642,   428, 13526,  3807,   804,   772,\n",
       "         19863,   922,    11,   475,  6411], device='cuda:0'),\n",
       " tensor([ 1212,  2646,   373,  7867,   416, 28440,   338,  1380,  2645,    78,\n",
       "            11], device='cuda:0')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "device = ppo_trainer.accelerator.device\n",
    "query = dataset[:4]['input_ids']\n",
    "response = ppo_trainer.generate(query, **generation_kwargs)\n",
    "response_tensors = [t.squeeze()[-gen_len:] for t in response]\n",
    "decoded_response = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_completions(batch):\n",
    "    tokenized_inputs = batch['input_ids']\n",
    "    queries = batch['query']\n",
    "    # For each input generate two responses.\n",
    "    response = ppo_trainer.generate(tokenized_inputs, **generation_kwargs)\n",
    "    response_tensors = [t.squeeze()[-gen_len:] for t in response]\n",
    "    decoded_response_1 = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "    \n",
    "    response = ppo_trainer.generate(tokenized_inputs, **generation_kwargs)\n",
    "    response_tensors = [t.squeeze()[-gen_len:] for t in response]\n",
    "    decoded_response_2 = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "    \n",
    "    # Collect the input and response for each.\n",
    "    response_1 = [q + r for q, r in zip(queries, decoded_response_1)]\n",
    "    response_2 = [q + r for q, r in zip(queries, decoded_response_2)]\n",
    "    \n",
    "    # Get the sentiment of response_1 and response_2 using bert sentiment classifier model.\n",
    "    pipe_outputs_1 = sentiment_pipe(response_1, **sent_kwargs)    \n",
    "    rewards_1 = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs_1]\n",
    "    \n",
    "    pipe_outputs_2 = sentiment_pipe(response_2, **sent_kwargs)\n",
    "    rewards_2 = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs_2]\n",
    "    \n",
    "    # Compute the preferred and rejected.\n",
    "    chosen = [r1 if t1.item() >= t2.item() else r2 for r1, r2, t1, t2 in zip(decoded_response_1, decoded_response_2, rewards_1, rewards_2)]\n",
    "    rejected = [r2 if t1.item() >= t2.item() else r1 for r1, r2, t1, t2 in zip(decoded_response_1, decoded_response_2, rewards_1, rewards_2)]\n",
    "    # return decoded_response_1, decoded_response_2, rewards_1, rewards_2, preferred, rejected\n",
    "    return {\n",
    "        'prompt': queries,\n",
    "        'chosen': chosen,\n",
    "        'rejected': rejected,\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch  = generate_completions(dataset[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "dpo_dataset = dataset.map(generate_completions, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    }
   ],
   "source": [
    "dpo_dataset.save_to_disk('dpo_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['review', 'label', 'input_ids', 'query', 'prompt', 'chosen', 'rejected'],\n",
       "    num_rows: 24895\n",
       "})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dpo_dataset2 = load_from_disk('dpo_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['review', 'label', 'input_ids', 'query', 'prompt', 'chosen', 'rejected'],\n",
       "    num_rows: 24895\n",
       "})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': ['I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       "  '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.',\n",
       "  \"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\",\n",
       "  \"This film was probably inspired by Godard's Masculin, féminin and I urge you to see that film instead.<br /><br />The film has two strong elements and those are, (1) the realistic acting (2) the impressive, undeservedly good, photo. Apart from that, what strikes me most is the endless stream of silliness. Lena Nyman has to be most annoying actress in the world. She acts so stupid and with all the nudity in this film,...it's unattractive. Comparing to Godard's film, intellectuality has been replaced with stupidity. Without going too far on this subject, I would say that follows from the difference in ideals between the French and the Swedish society.<br /><br />A movie of its time, and place. 2/10.\",\n",
       "  'Oh, brother...after hearing about this ridiculous film for umpteen years all I can think of is that old Peggy Lee song..<br /><br />\"Is that all there is??\" ...I was just an early teen when this smoked fish hit the U.S. I was too young to get in the theater (although I did manage to sneak into \"Goodbye Columbus\"). Then a screening at a local film museum beckoned - Finally I could see this film, except now I was as old as my parents were when they schlepped to see it!!<br /><br />The ONLY reason this film was not condemned to the anonymous sands of time was because of the obscenity case sparked by its U.S. release. MILLIONS of people flocked to this stinker, thinking they were going to see a sex film...Instead, they got lots of closeups of gnarly, repulsive Swedes, on-street interviews in bland shopping malls, asinie political pretension...and feeble who-cares simulated sex scenes with saggy, pale actors.<br /><br />Cultural icon, holy grail, historic artifact..whatever this thing was, shred it, burn it, then stuff the ashes in a lead box!<br /><br />Elite esthetes still scrape to find value in its boring pseudo revolutionary political spewings..But if it weren\\'t for the censorship scandal, it would have been ignored, then forgotten.<br /><br />Instead, the \"I Am Blank, Blank\" rhythymed title was repeated endlessly for years as a titilation for porno films (I am Curious, Lavender - for gay films, I Am Curious, Black - for blaxploitation films, etc..) and every ten years or so the thing rises from the dead, to be viewed by a new generation of suckers who want to see that \"naughty sex film\" that \"revolutionized the film industry\"...<br /><br />Yeesh, avoid like the plague..Or if you MUST see it - rent the video and fast forward to the \"dirty\" parts, just to get it over with.<br /><br />',\n",
       "  \"I would put this at the top of my list of films in the category of unwatchable trash! There are films that are bad, but the worst kind are the ones that are unwatchable but you are suppose to like them because they are supposed to be good for you! The sex sequences, so shocking in its day, couldn't even arouse a rabbit. The so called controversial politics is strictly high school sophomore amateur night Marxism. The film is self-consciously arty in the worst sense of the term. The photography is in a harsh grainy black and white. Some scenes are out of focus or taken from the wrong angle. Even the sound is bad! And some people call this art?<br /><br />\",\n",
       "  \"Whoever wrote the screenplay for this movie obviously never consulted any books about Lucille Ball, especially her autobiography. I've never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi. I could write a whole list of factual errors, but it would go on for pages. In all, I believe that Lucille Ball is one of those inimitable people who simply cannot be portrayed by anyone other than themselves. If I were Lucie Arnaz and Desi, Jr., I would be irate at how many mistakes were made in this film. The filmmakers tried hard, but the movie seems awfully sloppy to me.\",\n",
       "  'When I first saw a glimpse of this movie, I quickly noticed the actress who was playing the role of Lucille Ball. Rachel York\\'s portrayal of Lucy is absolutely awful. Lucille Ball was an astounding comedian with incredible talent. To think about a legend like Lucille Ball being portrayed the way she was in the movie is horrendous. I cannot believe out of all the actresses in the world who could play a much better Lucy, the producers decided to get Rachel York. She might be a good actress in other roles but to play the role of Lucille Ball is tough. It is pretty hard to find someone who could resemble Lucille Ball, but they could at least find someone a bit similar in looks and talent. If you noticed York\\'s portrayal of Lucy in episodes of I Love Lucy like the chocolate factory or vitavetavegamin, nothing is similar in any way-her expression, voice, or movement.<br /><br />To top it all off, Danny Pino playing Desi Arnaz is horrible. Pino does not qualify to play as Ricky. He\\'s small and skinny, his accent is unreal, and once again, his acting is unbelievable. Although Fred and Ethel were not similar either, they were not as bad as the characters of Lucy and Ricky.<br /><br />Overall, extremely horrible casting and the story is badly told. If people want to understand the real life situation of Lucille Ball, I suggest watching A&E Biography of Lucy and Desi, read the book from Lucille Ball herself, or PBS\\' American Masters: Finding Lucy. If you want to see a docudrama, \"Before the Laughter\" would be a better choice. The casting of Lucille Ball and Desi Arnaz in \"Before the Laughter\" is much better compared to this. At least, a similar aspect is shown rather than nothing.',\n",
       "  'Who are these \"They\"- the actors? the filmmakers? Certainly couldn\\'t be the audience- this is among the most air-puffed productions in existence. It\\'s the kind of movie that looks like it was a lot of fun to shoot\\x97 TOO much fun, nobody is getting any actual work done, and that almost always makes for a movie that\\'s no fun to watch.<br /><br />Ritter dons glasses so as to hammer home his character\\'s status as a sort of doppleganger of the bespectacled Bogdanovich; the scenes with the breezy Ms. Stratten are sweet, but have an embarrassing, look-guys-I\\'m-dating-the-prom-queen feel to them. Ben Gazzara sports his usual cat\\'s-got-canary grin in a futile attempt to elevate the meager plot, which requires him to pursue Audrey Hepburn with all the interest of a narcoleptic at an insomnia clinic. In the meantime, the budding couple\\'s respective children (nepotism alert: Bogdanovich\\'s daughters) spew cute and pick up some fairly disturbing pointers on \\'love\\' while observing their parents. (Ms. Hepburn, drawing on her dignity, manages to rise above the proceedings- but she has the monumental challenge of playing herself, ostensibly.) Everybody looks great, but so what? It\\'s a movie and we can expect that much, if that\\'s what you\\'re looking for you\\'d be better off picking up a copy of Vogue.<br /><br />Oh- and it has to be mentioned that Colleen Camp thoroughly annoys, even apart from her singing, which, while competent, is wholly unconvincing... the country and western numbers are woefully mismatched with the standards on the soundtrack. Surely this is NOT what Gershwin (who wrote the song from which the movie\\'s title is derived) had in mind; his stage musicals of the 20\\'s may have been slight, but at least they were long on charm. \"They All Laughed\" tries to coast on its good intentions, but nobody- least of all Peter Bogdanovich - has the good sense to put on the brakes.<br /><br />Due in no small part to the tragic death of Dorothy Stratten, this movie has a special place in the heart of Mr. Bogdanovich- he even bought it back from its producers, then distributed it on his own and went bankrupt when it didn\\'t prove popular. His rise and fall is among the more sympathetic and tragic of Hollywood stories, so there\\'s no joy in criticizing the film... there _is_ real emotional investment in Ms. Stratten\\'s scenes. But \"Laughed\" is a faint echo of \"The Last Picture Show\", \"Paper Moon\" or \"What\\'s Up, Doc\"- following \"Daisy Miller\" and \"At Long Last Love\", it was a thundering confirmation of the phase from which P.B. has never emerged.<br /><br />All in all, though, the movie is harmless, only a waste of rental. I want to watch people having a good time, I\\'ll go to the park on a sunny day. For filmic expressions of joy and love, I\\'ll stick to Ernest Lubitsch and Jaques Demy...',\n",
       "  \"This is said to be a personal film for Peter Bogdonavitch. He based it on his life but changed things around to fit the characters, who are detectives. These detectives date beautiful models and have no problem getting them. Sounds more like a millionaire playboy filmmaker than a detective, doesn't it? This entire movie was written by Peter, and it shows how out of touch with real people he was. You're supposed to write what you know, and he did that, indeed. And leaves the audience bored and confused, and jealous, for that matter. This is a curio for people who want to see Dorothy Stratten, who was murdered right after filming. But Patti Hanson, who would, in real life, marry Keith Richards, was also a model, like Stratten, but is a lot better and has a more ample part. In fact, Stratten's part seemed forced; added. She doesn't have a lot to do with the story, which is pretty convoluted to begin with. All in all, every character in this film is somebody that very few people can relate with, unless you're millionaire from Manhattan with beautiful supermodels at your beckon call. For the rest of us, it's an irritating snore fest. That's what happens when you're out of touch. You entertain your few friends with inside jokes, and bore all the rest.\"],\n",
       " 'label': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'input_ids': [tensor([   40, 26399,   314,  3001,   327, 47269, 20958]),\n",
       "  tensor([    1,    40,  1703, 44269,    25, 12550]),\n",
       "  tensor([1532,  691,  284, 3368, 1642,  428]),\n",
       "  tensor([1212, 2646]),\n",
       "  tensor([5812,   11, 3956,  986]),\n",
       "  tensor([ 40, 561]),\n",
       "  tensor([47896,  2630,   262, 39769,   329]),\n",
       "  tensor([ 2215,   314,   717,  2497,   257, 19350]),\n",
       "  tensor([8241,  389,  777,  366]),\n",
       "  tensor([1212,  318])],\n",
       " 'query': ['I rented I AM CURIOUS',\n",
       "  '\"I Am Curious: Yellow',\n",
       "  'If only to avoid making this',\n",
       "  'This film',\n",
       "  'Oh, brother...',\n",
       "  'I would',\n",
       "  'Whoever wrote the screenplay for',\n",
       "  'When I first saw a glimpse',\n",
       "  'Who are these \"',\n",
       "  'This is'],\n",
       " 'prompt': ['I rented I AM CURIOUS',\n",
       "  '\"I Am Curious: Yellow',\n",
       "  'If only to avoid making this',\n",
       "  'This film',\n",
       "  'Oh, brother...',\n",
       "  'I would',\n",
       "  'Whoever wrote the screenplay for',\n",
       "  'When I first saw a glimpse',\n",
       "  'Who are these \"',\n",
       "  'This is'],\n",
       " 'chosen': [\" OF YOU... WHO? Please don't try\",\n",
       "  ' Hawks\" also featured Jeanne Kaagin a strong',\n",
       "  ' a type of henchmen film: Even',\n",
       "  ' has been very popular and expressed our appreciation for',\n",
       "  'have mercy on future bride and me.\" (',\n",
       "  ' hardly bother wasting your talents whether it appeals to',\n",
       "  ' that movie are company well paid. The film',\n",
       "  ' of the Brony and Toy Eye, I',\n",
       "  'star reporters\" and not Judy Blanford',\n",
       "  ' a good watch with an undivided L'],\n",
       " 'rejected': [\" when it came out. There's no real\",\n",
       "  ' Moon In Pink Moon\" already has plenty of',\n",
       "  ' movie just hardcore, and very badly - I',\n",
       "  ' is certainly about the arguments against religious exclusion because',\n",
       "  ' even if it, you believe me, is',\n",
       "  ' think that if he could change what his plot',\n",
       "  ' \"Catwoman\". No, rather the director',\n",
       "  \" of this film, I guess I didn't\",\n",
       "  'lefty\" women looking for a job?',\n",
       "  ' exactly the argument I was after in trying to']}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_dataset2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dpo_dataset2 into train and test split.\n",
    "dpo_dataset_train_test = dpo_dataset2.train_test_split(test_size=0.1, shuffle=True)\n",
    "train_dataset = dpo_dataset_train_test['train']\n",
    "eval_dataset = dpo_dataset_train_test['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_steps = 1000\n",
    "gradient_accumulation_steps = 1\n",
    "learning_rate = 1e-3\n",
    "gradient_checkpointing = False\n",
    "beta = 0.1\n",
    "gen_len = output_length_sampler()\n",
    "max_new_tokens=gen_len\n",
    "max_target_length = 128\n",
    "max_prompt_length = 128\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    max_steps=max_steps,\n",
    "    remove_unused_columns=False,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_first_step=True,\n",
    "    logging_steps=10,  # match results in blog post\n",
    "    eval_steps=500,\n",
    "    output_dir=\"./test\",\n",
    "    optim=\"adamw_hf\",\n",
    "    warmup_steps=150,\n",
    "    report_to=None,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    # TODO: uncomment that on the next transformers release\n",
    "    # gradient_checkpointing_kwargs=script_args.gradient_checkpointing_kwargs,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(config.model_name)\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(config.model_name)\n",
    "\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    ref_model,\n",
    "    args=training_args,\n",
    "    beta=beta,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_new_tokens,\n",
    "    max_target_length=max_target_length,\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    generate_during_eval=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# End the current run\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 24:41, Epoch 11/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.344400</td>\n",
       "      <td>0.789826</td>\n",
       "      <td>-6.049232</td>\n",
       "      <td>-8.301433</td>\n",
       "      <td>0.371988</td>\n",
       "      <td>2.252200</td>\n",
       "      <td>-106.066620</td>\n",
       "      <td>-83.289574</td>\n",
       "      <td>-57.871819</td>\n",
       "      <td>-57.633175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.344500</td>\n",
       "      <td>0.835098</td>\n",
       "      <td>-6.833380</td>\n",
       "      <td>-9.375635</td>\n",
       "      <td>0.375635</td>\n",
       "      <td>2.542255</td>\n",
       "      <td>-116.808647</td>\n",
       "      <td>-91.131058</td>\n",
       "      <td>-60.792210</td>\n",
       "      <td>-60.542439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"<wandb.data_types.Table object at 0x7f91c7fc3b80>\" of type <class 'wandb.data_types.Table'> for key \"game_log\" as a metric. MLflow's log_metric() only accepts float and int types so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"<wandb.data_types.Table object at 0x7f91c7fc3b80>\" of type <class 'wandb.data_types.Table'> for key \"train/game_log\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"<wandb.data_types.Table object at 0x7f8fd47480d0>\" of type <class 'wandb.data_types.Table'> for key \"game_log\" as a metric. MLflow's log_metric() only accepts float and int types so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"<wandb.data_types.Table object at 0x7f8fd47480d0>\" of type <class 'wandb.data_types.Table'> for key \"train/game_log\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.3900393166542053, metrics={'train_runtime': 1514.1956, 'train_samples_per_second': 169.067, 'train_steps_per_second': 0.66, 'total_flos': 0.0, 'train_loss': 0.3900393166542053, 'epoch': 11.36})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = {\"min_length\": -1, \"top_k\": 0.0, \"top_p\": 1.0, \"do_sample\": True, \"pad_token_id\": tokenizer.eos_token_id}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpsingh/shopqa/jp-py310/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "/home/jpsingh/shopqa/jp-py310/lib/python3.10/site-packages/transformers/pipelines/base.py:1081: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>response (before)</th>\n",
       "      <th>response (after)</th>\n",
       "      <th>rewards (before)</th>\n",
       "      <th>rewards (after)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This film is probably</td>\n",
       "      <td>the funniest thing in the world,</td>\n",
       "      <td>best across and centre our films and Mer</td>\n",
       "      <td>2.612724</td>\n",
       "      <td>2.459077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"A death at</td>\n",
       "      <td>sea of 2008\". To own this movie is simply a s...</td>\n",
       "      <td>\"A death at increases increases in light&lt;|endo...</td>\n",
       "      <td>-2.593113</td>\n",
       "      <td>-0.222930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tim (Gary Daniels)</td>\n",
       "      <td>buys him back the money he pays him to give him</td>\n",
       "      <td>Tim (Gary Daniels) The witty&lt;|endoftext|&gt;</td>\n",
       "      <td>0.322140</td>\n",
       "      <td>0.619284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This movie</td>\n",
       "      <td>is really scary and deadly on it's own.&lt;</td>\n",
       "      <td>is witty &amp; and enjoyable. A well written&lt;|end...</td>\n",
       "      <td>2.484784</td>\n",
       "      <td>2.838642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This movie</td>\n",
       "      <td>I have saved 25 finales. I really wanted to</td>\n",
       "      <td>This movie is witty and compelling as its&lt;|end...</td>\n",
       "      <td>0.098681</td>\n",
       "      <td>2.596799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This film is the freshman</td>\n",
       "      <td>in the class and he comes up</td>\n",
       "      <td>the freshman to Michael Summersberg&lt;|endoftext|&gt;</td>\n",
       "      <td>0.852844</td>\n",
       "      <td>0.475493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tea Leon</td>\n",
       "      <td>, someone throws in a potion and tells Leon, \"</td>\n",
       "      <td>Tea Leon is witty&lt;|endoftext|&gt;</td>\n",
       "      <td>0.390674</td>\n",
       "      <td>1.388450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Man on Fire was hot.</td>\n",
       "      <td>Christine Simeone and Amy Poehler did</td>\n",
       "      <td>. It is accompanied by his wonderful performan...</td>\n",
       "      <td>1.409156</td>\n",
       "      <td>2.648448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The movie has one</td>\n",
       "      <td>decent working R-rated title</td>\n",
       "      <td>that is witty as a romantic</td>\n",
       "      <td>0.319576</td>\n",
       "      <td>2.515166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"The seventh sign\" borrows</td>\n",
       "      <td>heavily from the proverbs/memorabilia track \"</td>\n",
       "      <td>The seventh sign\" borrows elements from The Ch...</td>\n",
       "      <td>0.131347</td>\n",
       "      <td>0.039257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>One of those Thank-God</td>\n",
       "      <td>-for-Ice Ice cream commercials really got me</td>\n",
       "      <td>-loving people that you will never skipping a ...</td>\n",
       "      <td>1.469076</td>\n",
       "      <td>2.013238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>It is always a</td>\n",
       "      <td>always a pleasure to watch this movie over an...</td>\n",
       "      <td>It is always a very witty and innovative&lt;|endo...</td>\n",
       "      <td>2.473403</td>\n",
       "      <td>2.597743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Great little thriller</td>\n",
       "      <td>but decent film, it is good to</td>\n",
       "      <td>little thriller genre is amazing and's&lt;|endof...</td>\n",
       "      <td>2.324824</td>\n",
       "      <td>2.656723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Why would a person go back</td>\n",
       "      <td>over the lines to criticize their own boss an...</td>\n",
       "      <td>in 1968 when I saw that Wallace and Gs relate...</td>\n",
       "      <td>-1.601313</td>\n",
       "      <td>-0.235574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>This movie could be</td>\n",
       "      <td>termed a sloppy suspense drama. This</td>\n",
       "      <td>witty and witty thanks and witty as</td>\n",
       "      <td>-2.595539</td>\n",
       "      <td>2.259896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Yes, he is</td>\n",
       "      <td>a very positive actor</td>\n",
       "      <td>he is witty&lt;|endoftext|&gt;</td>\n",
       "      <td>2.431191</td>\n",
       "      <td>1.403419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         query  \\\n",
       "0        This film is probably   \n",
       "1                  \"A death at   \n",
       "2           Tim (Gary Daniels)   \n",
       "3                   This movie   \n",
       "4                   This movie   \n",
       "5    This film is the freshman   \n",
       "6                     Tea Leon   \n",
       "7         Man on Fire was hot.   \n",
       "8            The movie has one   \n",
       "9   \"The seventh sign\" borrows   \n",
       "10      One of those Thank-God   \n",
       "11              It is always a   \n",
       "12       Great little thriller   \n",
       "13  Why would a person go back   \n",
       "14         This movie could be   \n",
       "15                  Yes, he is   \n",
       "\n",
       "                                    response (before)  \\\n",
       "0                    the funniest thing in the world,   \n",
       "1    sea of 2008\". To own this movie is simply a s...   \n",
       "2     buys him back the money he pays him to give him   \n",
       "3            is really scary and deadly on it's own.<   \n",
       "4         I have saved 25 finales. I really wanted to   \n",
       "5                        in the class and he comes up   \n",
       "6      , someone throws in a potion and tells Leon, \"   \n",
       "7               Christine Simeone and Amy Poehler did   \n",
       "8                        decent working R-rated title   \n",
       "9       heavily from the proverbs/memorabilia track \"   \n",
       "10       -for-Ice Ice cream commercials really got me   \n",
       "11   always a pleasure to watch this movie over an...   \n",
       "12                     but decent film, it is good to   \n",
       "13   over the lines to criticize their own boss an...   \n",
       "14               termed a sloppy suspense drama. This   \n",
       "15                              a very positive actor   \n",
       "\n",
       "                                     response (after)  rewards (before)  \\\n",
       "0            best across and centre our films and Mer          2.612724   \n",
       "1   \"A death at increases increases in light<|endo...         -2.593113   \n",
       "2           Tim (Gary Daniels) The witty<|endoftext|>          0.322140   \n",
       "3    is witty & and enjoyable. A well written<|end...          2.484784   \n",
       "4   This movie is witty and compelling as its<|end...          0.098681   \n",
       "5    the freshman to Michael Summersberg<|endoftext|>          0.852844   \n",
       "6                      Tea Leon is witty<|endoftext|>          0.390674   \n",
       "7   . It is accompanied by his wonderful performan...          1.409156   \n",
       "8                         that is witty as a romantic          0.319576   \n",
       "9   The seventh sign\" borrows elements from The Ch...          0.131347   \n",
       "10  -loving people that you will never skipping a ...          1.469076   \n",
       "11  It is always a very witty and innovative<|endo...          2.473403   \n",
       "12   little thriller genre is amazing and's<|endof...          2.324824   \n",
       "13   in 1968 when I saw that Wallace and Gs relate...         -1.601313   \n",
       "14                witty and witty thanks and witty as         -2.595539   \n",
       "15                           he is witty<|endoftext|>          2.431191   \n",
       "\n",
       "    rewards (after)  \n",
       "0          2.459077  \n",
       "1         -0.222930  \n",
       "2          0.619284  \n",
       "3          2.838642  \n",
       "4          2.596799  \n",
       "5          0.475493  \n",
       "6          1.388450  \n",
       "7          2.648448  \n",
       "8          2.515166  \n",
       "9          0.039257  \n",
       "10         2.013238  \n",
       "11         2.597743  \n",
       "12         2.656723  \n",
       "13        -0.235574  \n",
       "14         2.259896  \n",
       "15         1.403419  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### get a batch from the dataset\n",
    "bs = 16\n",
    "game_data = dict()\n",
    "dataset.set_format(\"pandas\")\n",
    "df_batch = dataset[:].sample(bs)\n",
    "game_data[\"query\"] = df_batch[\"query\"].tolist()\n",
    "query_tensors = df_batch[\"input_ids\"].tolist()\n",
    "\n",
    "response_tensors_ref, response_tensors = [], []\n",
    "\n",
    "#### get response from gpt2 and gpt2_ref\n",
    "for i in range(bs):\n",
    "    gen_len = output_length_sampler()\n",
    "    output = ref_model.generate(\n",
    "        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    response_tensors_ref.append(output)\n",
    "    output = model.generate(\n",
    "        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    response_tensors.append(output)\n",
    "\n",
    "#### decode responses\n",
    "game_data[\"response (before)\"] = [tokenizer.decode(response_tensors_ref[i]) for i in range(bs)]\n",
    "game_data[\"response (after)\"] = [tokenizer.decode(response_tensors[i]) for i in range(bs)]\n",
    "\n",
    "#### sentiment analysis of query/response pairs before/after\n",
    "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (before)\"])]\n",
    "game_data[\"rewards (before)\"] = [output[1][\"score\"] for output in sentiment_pipe(texts, **sent_kwargs)]\n",
    "\n",
    "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (after)\"])]\n",
    "game_data[\"rewards (after)\"] = [output[1][\"score\"] for output in sentiment_pipe(texts, **sent_kwargs)]\n",
    "\n",
    "# store results in a dataframe\n",
    "df_results = pd.DataFrame(game_data)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rewards (before)    0.658153\n",
       "rewards (after)     1.628321\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "median:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rewards (before)    0.621759\n",
       "rewards (after)     2.136567\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"mean:\")\n",
    "display(df_results[[\"rewards (before)\", \"rewards (after)\"]].mean())\n",
    "print()\n",
    "print(\"median:\")\n",
    "display(df_results[[\"rewards (before)\", \"rewards (after)\"]].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jp-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
